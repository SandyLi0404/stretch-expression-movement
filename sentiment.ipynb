{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sandy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/sandy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/sandy/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /home/sandy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk # type: ignore\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "# The data fields are:\n",
    "# text: a string feature.\n",
    "# label: a classification label, with possible values including sadness (0), joy (1), love (2), anger (3), fear (4), surprise (5).\n",
    "\n",
    "file_path = \"data.txt\"\n",
    "sad = []\n",
    "joy = []\n",
    "anger = []\n",
    "surprise = []\n",
    "with open(file_path, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        if (re.search(\"0}$\", line) != None):\n",
    "            data = json.loads(line)\n",
    "            text_value = data[\"text\"]\n",
    "            sad.append(text_value)\n",
    "        elif (re.search(\"1}$\", line) != None):\n",
    "            data = json.loads(line)\n",
    "            text_value = data[\"text\"]\n",
    "            joy.append(text_value)\n",
    "        elif (re.search(\"3}$\", line) != None):\n",
    "            data = json.loads(line)\n",
    "            text_value = data[\"text\"]\n",
    "            anger.append(text_value)\n",
    "        elif (re.search(\"5}$\", line) != None):\n",
    "            data = json.loads(line)\n",
    "            text_value = data[\"text\"]\n",
    "            surprise.append(text_value)\n",
    "# print(anger[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "from nltk.tokenize import word_tokenize # type: ignore\n",
    "\n",
    "sad_tokens = []\n",
    "joy_tokens = []\n",
    "anger_tokens = []\n",
    "surprise_tokens = []\n",
    "\n",
    "for text in sad:\n",
    "    tokens = word_tokenize(text)\n",
    "    sad_tokens.append(tokens)\n",
    "\n",
    "for text in joy:\n",
    "    tokens = word_tokenize(text)\n",
    "    joy_tokens.append(tokens)\n",
    "\n",
    "for text in anger:\n",
    "    tokens = word_tokenize(text)\n",
    "    anger_tokens.append(tokens)\n",
    "\n",
    "for text in surprise:\n",
    "    tokens = word_tokenize(text)\n",
    "    surprise_tokens.append(tokens)\n",
    "\n",
    "# print(\"Original tweet:\")\n",
    "# print(anger[0])\n",
    "# print(\"--------------------\")\n",
    "# print(\"Tokenized tweet:\")\n",
    "# print(anger_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization & Noise Removal\n",
    "\n",
    "import string\n",
    "from nltk.tag import pos_tag # type: ignore\n",
    "from nltk.stem.wordnet import WordNetLemmatizer # type: ignore\n",
    "from nltk.corpus import stopwords # type: ignore\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def remove_noise(tokens, stop_words = ()):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens\n",
    "\n",
    "sad_tokens_preprocessed = []\n",
    "joy_tokens_preprocessed = []\n",
    "anger_tokens_preprocessed = []\n",
    "surprise_tokens_preprocessed = []\n",
    "\n",
    "for tokens in sad_tokens:\n",
    "    sad_tokens_preprocessed.append(remove_noise(tokens, stop_words))\n",
    "for tokens in joy_tokens:\n",
    "    joy_tokens_preprocessed.append(remove_noise(tokens, stop_words))\n",
    "for tokens in anger_tokens:\n",
    "    anger_tokens_preprocessed.append(remove_noise(tokens, stop_words))\n",
    "for tokens in surprise_tokens:\n",
    "    surprise_tokens_preprocessed.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "# print(sad_tokens_preprocessed[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the Naive Bayes Classifier\n",
    "\n",
    "# Convert to dictionary\n",
    "\n",
    "def convert_dict(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Labels\n",
    "sad_tokens_dict = convert_dict(sad_tokens_preprocessed)\n",
    "joy_tokens_dict = convert_dict(joy_tokens_preprocessed)\n",
    "anger_tokens_dict = convert_dict(anger_tokens_preprocessed)\n",
    "surprise_tokens_dict = convert_dict(surprise_tokens_preprocessed)\n",
    "\n",
    "sad_dataset = [(i, \"Sad\") for i in sad_tokens_dict]\n",
    "joy_dataset = [(tweet_dict, \"Joy\") for tweet_dict in joy_tokens_dict]\n",
    "anger_dataset = [(tweet_dict, \"Anger\") for tweet_dict in anger_tokens_dict]\n",
    "surprise_dataset = [(tweet_dict, \"Surprise\") for tweet_dict in surprise_tokens_dict]\n",
    "\n",
    "# print(sad_dataset[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.9231682251107316\n"
     ]
    }
   ],
   "source": [
    "# Combine Datasets\n",
    "import random\n",
    "dataset = sad_dataset + joy_dataset + anger_dataset + surprise_dataset\n",
    "random.shuffle(dataset)\n",
    "\n",
    "# Split Training & Testing\n",
    "train_data = dataset[:300000]\n",
    "test_data = dataset[300000:]\n",
    "\n",
    "# Build the Model\n",
    "from nltk import classify # type: ignore\n",
    "from nltk import NaiveBayesClassifier # type: ignore\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "# Test\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joy\n"
     ]
    }
   ],
   "source": [
    "# Sample - Using the Model\n",
    "\n",
    "# custom_tweet = \"I ordered just once from TerribleCo, they screwed up, never used the app again.\"\n",
    "custom_tweet = 'Congrats #SportStar on your 7th best goal from last season winning goal of the year :) #Baller #Topbin #oneofmanyworldies'\n",
    "\n",
    "custom_tokens = remove_noise(word_tokenize(custom_tweet))\n",
    "\n",
    "print(classifier.classify(dict([token, True] for token in custom_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampe2 = 'I hate you'\n",
    "custom_tokens = remove_noise(word_tokenize(sampe2))\n",
    "\n",
    "print(classifier.classify(dict([token, True] for token in custom_tokens)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
